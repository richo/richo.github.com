<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="generator" content="rustdoc">
    <title>The Rust Testing Guide</title>

    <link rel="stylesheet" type="text/css" href="rust.css">

    <link rel="shortcut icon" href="http://www.rust-lang.org/favicon.ico">
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400'
        rel='stylesheet' type='text/css'>

</head>
<body>
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

    <div id="versioninfo">
  <img src="http://www.rust-lang.org/logos/rust-logo-32x32-blk.png" width="32" height="32" alt><br>
  <span class="white-sticker"><a href="http://rust-lang.org">Rust</a> 0.11-pre</span><br>
  <a href="http://github.com/mozilla/rust/commit/1c05eafe98f478cbddde41b839a0c8673cf99554"
    class="hash white-sticker">1c05eafe</a>
</div>


    <h1 class="title">The Rust Testing Guide</h1>
    <nav id="TOC"><ul>
<li><a href="#quick-start">1 Quick start</a><ul></ul></li>
<li><a href="#unit-testing-in-rust">2 Unit testing in Rust</a><ul>
<li><a href="#parallelism">2.1 Parallelism</a><ul></ul></li>
<li><a href="#examples">2.2 Examples</a><ul>
<li><a href="#typical-test-run">2.2.1 Typical test run</a><ul></ul></li>
<li><a href="#test-run-with-failures">2.2.2 Test run with failures</a><ul></ul></li>
<li><a href="#running-ignored-tests">2.2.3 Running ignored tests</a><ul></ul></li>
<li><a href="#running-a-subset-of-tests">2.2.4 Running a subset of tests</a><ul></ul></li></ul></li></ul></li>
<li><a href="#microbenchmarking">3 Microbenchmarking</a><ul>
<li><a href="#benchmarks-and-the-optimizer">3.1 Benchmarks and the optimizer</a><ul></ul></li>
<li><a href="#saving-and-ratcheting-metrics">3.2 Saving and ratcheting metrics</a><ul></ul></li></ul></li></ul></nav>
<h1 id="quick-start" class='section-header'><a
                           href="#quick-start">1 Quick start</a></h1>
<p>To create test functions, add a <code>#[test]</code> attribute like this:</p>
<pre class='rust '>
<span class='kw'>fn</span> <span class='ident'>return_two</span>() <span class='op'>-&gt;</span> <span class='ident'>int</span> {
    <span class='number'>2</span>
}

<span class='attribute'>#[<span class='ident'>test</span>]</span>
<span class='kw'>fn</span> <span class='ident'>return_two_test</span>() {
    <span class='kw'>let</span> <span class='ident'>x</span> <span class='op'>=</span> <span class='ident'>return_two</span>();
    <span class='macro'>assert</span><span class='macro'>!</span>(<span class='ident'>x</span> <span class='op'>==</span> <span class='number'>2</span>);
}
</pre>

<p>To run these tests, compile with <code>rustc --test</code> and run the resulting
binary:</p>

<pre><code class="language-{.notrust}">$ rustc --test foo.rs
$ ./foo
running 1 test
test return_two_test ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured</code></pre>

<p><code>rustc foo.rs</code> will <em>not</em> compile the tests, since <code>#[test]</code> implies
<code>#[cfg(test)]</code>. The <code>--test</code> flag to <code>rustc</code> implies <code>--cfg test</code>.</p>

<h1 id="unit-testing-in-rust" class='section-header'><a
                           href="#unit-testing-in-rust">2 Unit testing in Rust</a></h1>
<p>Rust has built in support for simple unit testing. Functions can be
marked as unit tests using the <code>test</code> attribute.</p>
<pre class='rust '>
<span class='attribute'>#[<span class='ident'>test</span>]</span>
<span class='kw'>fn</span> <span class='ident'>return_none_if_empty</span>() {<span class='comment'>
    // ... test code ...
</span>}
</pre>

<p>A test function&#39;s signature must have no arguments and no return
value. To run the tests in a crate, it must be compiled with the
<code>--test</code> flag: <code>rustc myprogram.rs --test -o myprogram-tests</code>. Running
the resulting executable will run all the tests in the crate. A test
is considered successful if its function returns; if the task running
the test fails, through a call to <code>fail!</code>, a failed <code>assert</code>, or some
other (<code>assert_eq</code>, ...) means, then the test fails.</p>

<p>When compiling a crate with the <code>--test</code> flag <code>--cfg test</code> is also
implied, so that tests can be conditionally compiled.</p>
<pre class='rust '>
<span class='attribute'>#[<span class='ident'>cfg</span>(<span class='ident'>test</span>)]</span>
<span class='kw'>mod</span> <span class='ident'>tests</span> {
    <span class='attribute'>#[<span class='ident'>test</span>]</span>
    <span class='kw'>fn</span> <span class='ident'>return_none_if_empty</span>() {<span class='comment'>
      // ... test code ...
    </span>}
}
</pre>

<p>Additionally <code>#[test]</code> items behave as if they also have the
<code>#[cfg(test)]</code> attribute, and will not be compiled when the <code>--test</code> flag
is not used.</p>

<p>Tests that should not be run can be annotated with the <code>ignore</code>
attribute. The existence of these tests will be noted in the test
runner output, but the test will not be run. Tests can also be ignored
by configuration so, for example, to ignore a test on windows you can
write <code>#[ignore(cfg(target_os = &quot;win32&quot;))]</code>.</p>

<p>Tests that are intended to fail can be annotated with the
<code>should_fail</code> attribute. The test will be run, and if it causes its
task to fail then the test will be counted as successful; otherwise it
will be counted as a failure. For example:</p>
<pre class='rust '>
<span class='attribute'>#[<span class='ident'>test</span>]</span>
<span class='attribute'>#[<span class='ident'>should_fail</span>]</span>
<span class='kw'>fn</span> <span class='ident'>test_out_of_bounds_failure</span>() {
    <span class='kw'>let</span> <span class='ident'>v</span>: [<span class='ident'>int</span>] <span class='op'>=</span> [];
    <span class='ident'>v</span>[<span class='number'>0</span>];
}
</pre>

<p>A test runner built with the <code>--test</code> flag supports a limited set of
arguments to control which tests are run: the first free argument
passed to a test runner specifies a filter used to narrow down the set
of tests being run; the <code>--ignored</code> flag tells the test runner to run
only tests with the <code>ignore</code> attribute.</p>

<h2 id="parallelism" class='section-header'><a
                           href="#parallelism">2.1 Parallelism</a></h2>
<p>By default, tests are run in parallel, which can make interpreting
failure output difficult. In these cases you can set the
<code>RUST_TEST_TASKS</code> environment variable to 1 to make the tests run
sequentially.</p>

<h2 id="examples" class='section-header'><a
                           href="#examples">2.2 Examples</a></h2>
<h3 id="typical-test-run" class='section-header'><a
                           href="#typical-test-run">2.2.1 Typical test run</a></h3>
<pre><code class="language-{.notrust}">$ mytests

running 30 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest2 ... ignored
... snip ...
running driver::tests::mytest30 ... ok

result: ok. 28 passed; 0 failed; 2 ignored</code></pre>

<h3 id="test-run-with-failures" class='section-header'><a
                           href="#test-run-with-failures">2.2.2 Test run with failures</a></h3>
<pre><code class="language-{.notrust}">$ mytests

running 30 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest2 ... ignored
... snip ...
running driver::tests::mytest30 ... FAILED

result: FAILED. 27 passed; 1 failed; 2 ignored</code></pre>

<h3 id="running-ignored-tests" class='section-header'><a
                           href="#running-ignored-tests">2.2.3 Running ignored tests</a></h3>
<pre><code class="language-{.notrust}">$ mytests --ignored

running 2 tests
running driver::tests::mytest2 ... failed
running driver::tests::mytest10 ... ok

result: FAILED. 1 passed; 1 failed; 0 ignored</code></pre>

<h3 id="running-a-subset-of-tests" class='section-header'><a
                           href="#running-a-subset-of-tests">2.2.4 Running a subset of tests</a></h3>
<pre><code class="language-{.notrust}">$ mytests mytest1

running 11 tests
running driver::tests::mytest1 ... ok
running driver::tests::mytest10 ... ignored
... snip ...
running driver::tests::mytest19 ... ok

result: ok. 11 passed; 0 failed; 1 ignored</code></pre>

<h1 id="microbenchmarking" class='section-header'><a
                           href="#microbenchmarking">3 Microbenchmarking</a></h1>
<p>The test runner also understands a simple form of benchmark execution.
Benchmark functions are marked with the <code>#[bench]</code> attribute, rather
than <code>#[test]</code>, and have a different form and meaning. They are
compiled along with <code>#[test]</code> functions when a crate is compiled with
<code>--test</code>, but they are not run by default. To run the benchmark
component of your testsuite, pass <code>--bench</code> to the compiled test
runner.</p>

<p>The type signature of a benchmark function differs from a unit test:
it takes a mutable reference to type
<code>test::Bencher</code>. Inside the benchmark function, any
time-variable or &quot;setup&quot; code should execute first, followed by a call
to <code>iter</code> on the benchmark harness, passing a closure that contains
the portion of the benchmark you wish to actually measure the
per-iteration speed of.</p>

<p>For benchmarks relating to processing/generating data, one can set the
<code>bytes</code> field to the number of bytes consumed/produced in each
iteration; this will used to show the throughput of the benchmark.
This must be the amount used in each iteration, <em>not</em> the total
amount.</p>

<p>For example:</p>
<pre class='rust '>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>test</span>;

<span class='kw'>use</span> <span class='ident'>std</span>::<span class='ident'>slice</span>;
<span class='kw'>use</span> <span class='ident'>test</span>::<span class='ident'>Bencher</span>;

<span class='attribute'>#[<span class='ident'>bench</span>]</span>
<span class='kw'>fn</span> <span class='ident'>bench_sum_1024_ints</span>(<span class='ident'>b</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Bencher</span>) {
    <span class='kw'>let</span> <span class='ident'>v</span> <span class='op'>=</span> <span class='ident'>slice</span>::<span class='ident'>from_fn</span>(<span class='number'>1024</span>, <span class='op'>|</span><span class='ident'>n</span><span class='op'>|</span> <span class='ident'>n</span>);
    <span class='ident'>b</span>.<span class='ident'>iter</span>(<span class='op'>||</span> {<span class='ident'>v</span>.<span class='ident'>iter</span>().<span class='ident'>fold</span>(<span class='number'>0</span>, <span class='op'>|</span><span class='ident'>old</span>, <span class='ident'>new</span><span class='op'>|</span> <span class='ident'>old</span> <span class='op'>+</span> <span class='op'>*</span><span class='ident'>new</span>);} );
}

<span class='attribute'>#[<span class='ident'>bench</span>]</span>
<span class='kw'>fn</span> <span class='ident'>initialise_a_vector</span>(<span class='ident'>b</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Bencher</span>) {
    <span class='ident'>b</span>.<span class='ident'>iter</span>(<span class='op'>||</span> {<span class='ident'>slice</span>::<span class='ident'>from_elem</span>(<span class='number'>1024</span>, <span class='number'>0u64</span>);} );
    <span class='ident'>b</span>.<span class='ident'>bytes</span> <span class='op'>=</span> <span class='number'>1024</span> <span class='op'>*</span> <span class='number'>8</span>;
}
</pre>

<p>The benchmark runner will calibrate measurement of the benchmark
function to run the <code>iter</code> block &quot;enough&quot; times to get a reliable
measure of the per-iteration speed.</p>

<p>Advice on writing benchmarks:</p>

<ul>
<li>Move setup code outside the <code>iter</code> loop; only put the part you
want to measure inside</li>
<li>Make the code do &quot;the same thing&quot; on each iteration; do not
accumulate or change state</li>
<li>Make the outer function idempotent too; the benchmark runner is
likely to run it many times</li>
<li>Make the inner <code>iter</code> loop short and fast so benchmark runs are
fast and the calibrator can adjust the run-length at fine
resolution</li>
<li>Make the code in the <code>iter</code> loop do something simple, to assist in
pinpointing performance improvements (or regressions)</li>
</ul>

<p>To run benchmarks, pass the <code>--bench</code> flag to the compiled
test-runner. Benchmarks are compiled-in but not executed by default.</p>

<pre><code class="language-{.notrust}">$ rustc mytests.rs -O --test
$ mytests --bench

running 2 tests
test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)
test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s

test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured</code></pre>

<h2 id="benchmarks-and-the-optimizer" class='section-header'><a
                           href="#benchmarks-and-the-optimizer">3.1 Benchmarks and the optimizer</a></h2>
<p>Benchmarks compiled with optimizations activated can be dramatically
changed by the optimizer so that the benchmark is no longer
benchmarking what one expects. For example, the compiler might
recognize that some calculation has no external effects and remove
it entirely.</p>
<pre class='rust '>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>test</span>;
<span class='kw'>use</span> <span class='ident'>test</span>::<span class='ident'>Bencher</span>;

<span class='attribute'>#[<span class='ident'>bench</span>]</span>
<span class='kw'>fn</span> <span class='ident'>bench_xor_1000_ints</span>(<span class='ident'>b</span>: <span class='kw-2'>&amp;</span><span class='kw-2'>mut</span> <span class='ident'>Bencher</span>) {
    <span class='ident'>b</span>.<span class='ident'>iter</span>(<span class='op'>||</span> {
            <span class='ident'>range</span>(<span class='number'>0</span>, <span class='number'>1000</span>).<span class='ident'>fold</span>(<span class='number'>0</span>, <span class='op'>|</span><span class='ident'>old</span>, <span class='ident'>new</span><span class='op'>|</span> <span class='ident'>old</span> <span class='op'>^</span> <span class='ident'>new</span>);
        });
}
</pre>

<p>gives the following results</p>

<pre><code class="language-{.notrust}">running 1 test
test bench_xor_1000_ints ... bench:         0 ns/iter (+/- 0)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured</code></pre>

<p>The benchmarking runner offers two ways to avoid this. Either, the
closure that the <code>iter</code> method receives can return an arbitrary value
which forces the optimizer to consider the result used and ensures it
cannot remove the computation entirely. This could be done for the
example above by adjusting the <code>bh.iter</code> call to</p>
<pre class='rust '>
<span class='ident'>bh</span>.<span class='ident'>iter</span>(<span class='op'>||</span> <span class='ident'>range</span>(<span class='number'>0</span>, <span class='number'>1000</span>).<span class='ident'>fold</span>(<span class='number'>0</span>, <span class='op'>|</span><span class='ident'>old</span>, <span class='ident'>new</span><span class='op'>|</span> <span class='ident'>old</span> <span class='op'>^</span> <span class='ident'>new</span>))
</pre>

<p>Or, the other option is to call the generic <code>test::black_box</code>
function, which is an opaque &quot;black box&quot; to the optimizer and so
forces it to consider any argument as used.</p>
<pre class='rust '>
<span class='kw'>extern</span> <span class='kw'>crate</span> <span class='ident'>test</span>;

<span class='ident'>bh</span>.<span class='ident'>iter</span>(<span class='op'>||</span> {
        <span class='ident'>test</span>::<span class='ident'>black_box</span>(<span class='ident'>range</span>(<span class='number'>0</span>, <span class='number'>1000</span>).<span class='ident'>fold</span>(<span class='number'>0</span>, <span class='op'>|</span><span class='ident'>old</span>, <span class='ident'>new</span><span class='op'>|</span> <span class='ident'>old</span> <span class='op'>^</span> <span class='ident'>new</span>));
    });
</pre>

<p>Neither of these read or modify the value, and are very cheap for
small values. Larger values can be passed indirectly to reduce
overhead (e.g. <code>black_box(&amp;huge_struct)</code>).</p>

<p>Performing either of the above changes gives the following
benchmarking results</p>

<pre><code class="language-{.notrust}">running 1 test
test bench_xor_1000_ints ... bench:       375 ns/iter (+/- 148)

test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured</code></pre>

<p>However, the optimizer can still modify a testcase in an undesirable
manner even when using either of the above. Benchmarks can be checked
by hand by looking at the output of the compiler using the <code>--emit=ir</code>
(for LLVM IR), <code>--emit=asm</code> (for assembly) or compiling normally and
using any method for examining object code.</p>

<h2 id="saving-and-ratcheting-metrics" class='section-header'><a
                           href="#saving-and-ratcheting-metrics">3.2 Saving and ratcheting metrics</a></h2>
<p>When running benchmarks or other tests, the test runner can record
per-test &quot;metrics&quot;. Each metric is a scalar <code>f64</code> value, plus a noise
value which represents uncertainty in the measurement. By default, all
<code>#[bench]</code> benchmarks are recorded as metrics, which can be saved as
JSON in an external file for further reporting.</p>

<p>In addition, the test runner supports <em>ratcheting</em> against a metrics
file. Ratcheting is like saving metrics, except that after each run,
if the output file already exists the results of the current run are
compared against the contents of the existing file, and any regression
<em>causes the testsuite to fail</em>. If the comparison passes -- if all
metrics stayed the same (within noise) or improved -- then the metrics
file is overwritten with the new values. In this way, a metrics file
in your workspace can be used to ensure your work does not regress
performance.</p>

<p>Test runners take 3 options that are relevant to metrics:</p>

<ul>
<li><code>--save-metrics=&lt;file.json&gt;</code> will save the metrics from a test run
to <code>file.json</code></li>
<li><code>--ratchet-metrics=&lt;file.json&gt;</code> will ratchet the metrics against
the <code>file.json</code></li>
<li><code>--ratchet-noise-percent=N</code> will override the noise measurements
in <code>file.json</code>, and consider a metric change less than <code>N%</code> to be
noise. This can be helpful if you are testing in a noisy
environment where the benchmark calibration loop cannot acquire a
clear enough signal.</li>
</ul>

    <footer><p>
Copyright &copy; 2011-2014 The Rust Project Developers. Licensed under the
<a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License, Version 2.0</a>
or the <a href="http://opensource.org/licenses/MIT">MIT license</a>, at your option.
</p><p>
This file may not be copied, modified, or distributed except according to those terms.
</p></footer>


</body>
</html>